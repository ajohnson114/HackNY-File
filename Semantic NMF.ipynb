{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f264ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from copy import deepcopy\n",
    "from pprint import pprint\n",
    "import scipy\n",
    "import math\n",
    "from pprint import pprint\n",
    "\n",
    "def step(W,H,S):\n",
    "    \"\"\"This is a direct implementation of the Multiplicative Update \n",
    "    step detailed in the paper but edited to our projects syntax\n",
    "    \n",
    "    Args:\n",
    "        W = Dictionary matrix In paper: Z In our project: W\n",
    "        H = Coding matrix In paper: W^t In our project: H\n",
    "        S = Symmetric Matrix that is randomly defined same in both the paper and our project\n",
    "    Input: Numpy arrays Output: Updated Numpy Arrays\n",
    "    The dimensions should not change\"\"\"\n",
    "    Ht = H.T\n",
    "    W = W * ((X @ Ht)/((W @ H @ Ht)+10**-9))\n",
    "    Ht = Ht * ((X.T @ W + (2 * M @ Ht @ S))/((Ht @ ((2 * S @ H @ Ht @ S)+ W.T @ W))+10**-9))\n",
    "    S = S * (H @ M @ Ht)/(H @ Ht @ S @ H @ Ht)\n",
    "    return W, Ht.T, S\n",
    "\n",
    "\n",
    "#Function to be minimized\n",
    "def loss_fn(X,W,H,M,S):\n",
    "    \"\"\"This is the function that needed to be optimized as per the paper.\n",
    "    Input: numpy arrays Output: scalar loss value\"\"\"\n",
    "    Ht = H.T\n",
    "    loss1 = (np.linalg.norm((X - (W @ H)), 'fro')**2)\n",
    "    loss2 = (np.linalg.norm((M - (Ht @ S @ H)),'fro')**2)\n",
    "    loss = (1/2)*(loss1+loss2)\n",
    "    return loss\n",
    "\n",
    "def shifted_PPMI(documents, important_vocab, word_window = 0, neg_sample = 0):\n",
    "    \"\"\"This is supposed to caluclate the Shifted PPMI which \n",
    "    becomes the M used in Semantic NMF. It is based off the skip-gram\n",
    "    model so neg_sample is the amount of negative sampling the researcher\n",
    "    would use. For large corpora: 2-3 is ideal, for small: 5-20.\n",
    "\n",
    "    Input: Preprocessed text in the form of documents (take out punctuation, etc) , important vocab (words in the X matrix),\n",
    "    word window, and amount of negative sampling. Important vocab should be something that's iterable (list,tuple,etc)\n",
    "    and the code is designed to be composed of the most important words in the TF-IDF perspective due to\n",
    "    computation constraints\n",
    "\n",
    "    Output: M as a numpy array of dim: important_vocab x important_vocab\"\"\"\n",
    "    data = []\n",
    "    Omega = []\n",
    "    k = neg_sample\n",
    "    word_window = word_window\n",
    "    \n",
    "    for document in documents:\n",
    "        array = []\n",
    "        for word in document.split():\n",
    "            array.append(word)\n",
    "        data.append(array)\n",
    "    #Gets all the words without punctuation into a list \n",
    "    #for line in documents:\n",
    "        #for word in line.split():\n",
    "            #data.append(word)\n",
    "\n",
    "    \n",
    "    #Getting the amount of times the words appear within the word window\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            for k in range(word_window):\n",
    "                if j + k + 1  <= len(data[i][j]):\n",
    "                    Omega.append((data[i][j],data[i][j+k]))\n",
    "                else:\n",
    "                    pass\n",
    "                if j - k < 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    Omega.append((data[i][j],data[i][j-k]))\n",
    "                \n",
    "    #Counting total occurences of word pairings and individual words\n",
    "    from collections import defaultdict\n",
    "    Nij_counts = defaultdict(int)\n",
    "    for (i,j) in Omega:\n",
    "        Nij_counts[ (i,j) ] += 1\n",
    "    #P(i) = Ni_counts/N and the same for j\n",
    "    Ni_counts = defaultdict(int)\n",
    "    Nj_counts = defaultdict(int)\n",
    "    for (i,j), N_ij in Nij_counts.items():\n",
    "        Ni_counts[ i ] += N_ij\n",
    "        Nj_counts[ j ] += N_ij\n",
    "    \n",
    "    #Loading words into a data frame and dictionary\n",
    "    # Now we are loading in the data from the dataframe (quick and kind of dirty)\n",
    "    vocab = {}\n",
    "    invvocab = important_vocab\n",
    "\n",
    "    # count for words, not super efficient\n",
    "    #for i in sorted(Ni_counts.items(), key=lambda kv: kv[1]): #Sorts and gives max to min for values a\n",
    "    for i in important_vocab:\n",
    "        vocab[i] = Ni_counts[i]\n",
    "        \n",
    "    V = len(invvocab)\n",
    "    print(\"V = {}\".format(V))\n",
    "    # Given the vocabulary mapping above, we now fill in the Nij matrix.\n",
    "    # NOTE - with large vocabs, this should be done using a *sparse* matrix!!!\n",
    "    from scipy.sparse import csr_matrix\n",
    "    Nij_np = csr_matrix((V, V))\n",
    "    for m in range(V):\n",
    "        i = invvocab[m]\n",
    "        for n in range(V):\n",
    "            j = invvocab[n]\n",
    "            Nij_np[m, n] = Nij_counts[(i,j)]\n",
    "    \n",
    "    # marginalize to get the unigram counts\n",
    "    Nij = tf.convert_to_tensor(Nij_np.todense(), dtype=tf.float32)\n",
    "    N = tf.reduce_sum(Nij)\n",
    "    Ni = tf.reduce_sum(Nij, axis=0)\n",
    "    Nj = tf.reduce_sum(Nij, axis=1)\n",
    "\n",
    "    PMI_np = tf.math.log((N * Nij) / (tf.einsum('i,j->ij', Ni, Nj)*k))\n",
    "    PMI_np = tf.Session().run(PMI_np)\n",
    "    shifted_PPMI = deepcopy(PMI_np)\n",
    "                      \n",
    "    for i in range(V):\n",
    "        for j in range(V):\n",
    "            if shifted_PPMI[i][j] < 0 or np.isnan(shifted_PPMI[i][j]):\n",
    "                shifted_PPMI[i][j] = 0 \n",
    "    \n",
    "\n",
    "    return shifted_PPMI\n",
    "\n",
    "    #Shifted PMI formula equivalent\n",
    "    #log((N*Nij)/(NiNj*k))\n",
    "    #N = Amount of words\n",
    "    #Nij = Amount of cooccurrences\n",
    "    #Ni (Nj) = Amount of occurrences of i (j)\n",
    "    #k = Amount of negative samples\n",
    "    \n",
    "def Semantic_NMF(num_iterations,X,W,H,M,S):\n",
    "    \"\"\"Shows your current loss and allows you to specify how many times to update your matrices\n",
    "    Input: All the matrices and the amount of times you want to update W,H, and S\n",
    "    Output: Updated versions of W,H,S\n",
    "    \"\"\"\n",
    "    for i in range(num_iterations):\n",
    "        if i % 100 == 0:\n",
    "            print(\"Your current loss is {:0.8f}\".format(loss_fn(X,W,H,M,S)))\n",
    "        W, H ,S = step(W,H,S)\n",
    "    return W,H,S\n",
    "\n",
    "def highest_PMI_words(amount):\n",
    "    \"\"\" \n",
    "    Note that there was a slight bug when extracting corpus statistics,\n",
    "    since the context window (w=5) is symmetric, PMI(i,j) should always equal\n",
    "    PMI(j,i); however, due to improper handling of context during the first 5 words\n",
    "    of a document, the statistics are ever-so-slightly distorted.\n",
    "    \n",
    "    Input: Amount of words you want printed with PMI values in descending order.\n",
    "    Output: Prints word pairs with highest PMI values (neglects same word pairings ala (hello,hello)) \n",
    "    \"\"\"\n",
    "    printed = [] \n",
    "    def not_printed(term, context, printed):\n",
    "        if (term,context) or (context,term) in printed:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    \n",
    "    # look at the biggest PMIs\n",
    "    sorted_ind = np.argsort(M, axis=None)\n",
    "\n",
    "    print('--- Word pairs with the notable PMIs ---\\n')\n",
    "    for i in range(1,amount):\n",
    "        ind = np.unravel_index(sorted_ind[-i], M.shape)\n",
    "        term, context = important_vocab[ind[0]], important_vocab[ind[1]]\n",
    "        if term != context and not_printed(term, context, printed):\n",
    "            printed.append((term,context))\n",
    "            print('{:8} {:14} (PMI = {:0.4f})'.format(term, context, float(M[ind])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
